---
layout: post
title:  "Machine Learning workflow"
categories: machine-learning
tags: machine-learning artificial-intelligence ai ml-beginner
---

Given the huge amount of information available to study ML, it's easy to get lost at times. How does everything fit together?

A simple way to help is having a **high level workflow** view with the different steps you would usually take when solving a new problem using ML. There is no one-size-fits all and there are many variations on such a flow, but *if you're getting started*, this type of structure can help you map all the learnings back to a 'place' and see them 'from above' to understand how they fit into the overall scheme, or even look into what 'steps' you haven't done much learning yet on.

<sup>* Found a tool to define *structured flows* for your modeling pipeline that might be worth exploring in the future [fn_graph](https://fn_graph.businessoptics.biz/).</sup>

Here's the simplistic view I'm using for my own learning:
![ML Cycle](/assets/ml-cycle.png)

Here's a high level breakdown of each step.

## ü§î Understand the problem 
I'd skip this if you're just starting to learn ML and experimenting, but you will surely do this step for real world projects. Nonetheless, it's always good to exercise some **critical thinking** before you start a project.

{::options parse_block_html="true" /}

<details>
<summary markdown='span'>
Things to consider:
</summary>

* What problem are you trying to solve? Is it well defined?
* What are you trying to get out of the model and how does it align with objectives?
* Do you have enough data to train the model?
* Are there better alternatives other than ML?
* How would you define and measure the model success in order to consider it 'ready' for production?
</details>

{::options parse_block_html="false" /}

## üìä Explore the data
This step is all about investigating the available data to understand it better. You will need some good **plotting tools** and **statistical analysis** skills at this point to quickly plot data in an understandable way, look at data distribution, correlations, etc.

{::options parse_block_html="true" /}
<details>
<summary markdown='span'>
Things to consider:
</summary>

- Where does the data come from? How many data sources?
- What is the type (e.g. numerical, categorical, etc) and distribution of each feature?
- What are the relationships/trends between different features?
- From a timeline perspective, when is each feature added to the dataset?
- What is the potential bias in the data? How to guarantee a fair dataset for the given objectives?
- How much work to consolidate it into one dataset for model training?
- Are there many features? Which ones 'feel' more relevant after analysis? Which ones could be dropped?
</details>

{::options parse_block_html="false" /}

## üßπ Clean the data
After you look at the data and understand it better, you'll also have seen all the problems with it. Usually it will be things like missing values, features with incorrect data types, skewed data, outlier values etc.

{::options parse_block_html="true" /}
<details>
<summary markdown='span'>
Things to consider:
</summary>

- What to replace missing values with? (Imputation)
- Can any features be dropped? How much valuable data are we losing?
- What features **should** be dropped? (e.g. don't use features that are added to the data at the same time the one we're trying to predict ‚Äîthese feature will be highly predictive in testing, but in production the feature won't exist so the model performs poorly)
</details>

{::options parse_block_html="false" /}

## üõ† Feature engineering
This is the first part of the **model fine-tuning cycle**. If it's the first time you're doing this step, don't do any feature engineering, just train a model with the current features, validate it and use that as a baseline for further improvements.

This step is about how to manipulate the available features so they contribute to improve the model outcomes. There are so many things to consider here, and they all impact the model accuracy, it can feel daunting. Start small, iterate often and have a way to compare the outcome of the changes with previous iterations.

{::options parse_block_html="true" /}
<details>
<summary markdown='span'>
Things to consider:
</summary>

- Creating new features out of existing ones, e.g. split date into day, month, year, is weekend, is end of quarter, etc. if it makes sense for the problem at hand
- Transforming data types
- Normalising numerical data distribution
- Formatting data e.g. strip unnecessary text
- Encoding of categorical features (many different types of encodings are available)
- How many features are too many? How does this relate to model underfitting / overfitting?
- Have new features created shown to have low impact on accuracy? Do we keep them?
</details>

{::options parse_block_html="false" /}

I believe that at the end of this stage you should also have a **list of relevant features to be used when training** the model. You'll learn as you tune the model that not all features are as relevant as they seem. It's a fine art to find the smallest number of features that lead to the best results.

These actions will also depend on the type of model you're trying to train, since some models work well with categorical data as is, others need it to be converted to numerical, etc.

## ‚úÇÔ∏è Split the data
What data will you use to **train**, **validate** and **test** your model? This is such a simple question, yet it will have a huge impact on the accuracy of the model if not addressed properly. Once you decide on a splitting strategy, it's common it won't change often so this step can be skipped.

{::options parse_block_html="true" /}
<details>
<summary markdown='span'>
Things to consider:
</summary>

- Why is splitting data into training, validation and testing sets important?
- What are different techniques to split your dataset? (% split, cross-validation, etc.) Which ones are better for your dataset size?
- Are the training and validation datasets balanced in terms of the data in them?
- Is the test dataset well balanced with the most expected cases when running live?
- How can you prevent data leakage between the splits?
</details>

{::options parse_block_html="false" /}

## üèãÔ∏è‚Äç‚ôÇÔ∏è Train the model
This step is about **selecting a model**, **feeding it the training data along with the expected training results**, and have it figure out what the best way to map the data to the results is (super simplistic view).

{::options parse_block_html="true" /}
<details>
<summary markdown='span'>
Things to consider:
</summary>

- What model to choose, if you haven't yet? What other models could be experimented with?
- What are the characteristics of the model regarding data input and tuning parameters?
- Does the model tend to behave better or worse for the current features? (e.g. does it need all numerical input Vs works ok with categorical data, does it need normalised numerical data, etc.)
- What parameters are available to tune the model and how do they influence it?
- What features from all the available will you want to use?
- Am I using pipelines (sklearn), can I put that in place now?
</details>

{::options parse_block_html="false" /}

## ‚úÖ Validate the model
Validating the model that was trained before is how you will measure its accuracy to solve the problem at hand. At this stage, you **feed the model with the validation data**, get the output and **calculate an accuracy metric** against the expected validation set outputs.

{::options parse_block_html="true" /}
<details>
<summary markdown='span'>
Things to consider:
</summary>

- What accuracy metric calculation methods exist? (e.g. mean absolute error)
- Which ones are better to measure this particular model output Vs expected output?
- Are there any possibilities of data leakage?
- Are you validating **just** on validation data?
</details>

{::options parse_block_html="false" /}

After this step, we should be able to demonstrate that there have been improvements on the accuracy of the model over this last iteration of **Feature Engineering** > **Split Data** > **Train Model** > **Validate Model** cycle.

A few things can happen at the end of this stage:
- Continue fine tunning this model via more feature engineering, feature selection, model hyperparameter tunning, keeping the ones that improve accuracy (careful with underfitting/overfitting)
- Experiment with new models (ensembles, etc.) and see if there are relevant improvements in accuracy
- If the current accuracy is at a level that has been defined as good for the problem at hand, then the next step would be to run the model against a **test dataset**, and then assess deploying it to production.

## üß™ Test the model
After tuning the model with the training and validation data to achieve satisfactory results, you can then run your model against the test dataset. The key difference here is that **the test dataset was never seen by the model at any point in time**, and it has also been built carefully in a way that is balanced and contains most expected cases in real life.

This step should be just like validation, but with the test dataset. **Feed the test data to the model** and **get the output**, then **compare against the expected results** according to the chosen accuracy metric. If it has better accuracy, or is more performant with similar accuracy, you should consider it for deployment into a live system.

On the other end, if the testing results are worse than previously, you have to go back to the drawing board and start the cycle again.

## üôè Deploy
Won't go into detail here for now, but this step would be about versioning your model and deploying it to a production environment to be used in real life. Good Luck! :)